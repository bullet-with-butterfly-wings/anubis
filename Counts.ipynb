{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['proAnubis_240810_0840.raw', 'proAnubis_240731_0817.raw', 'proAnubis_240731_0217.raw', 'proAnubis_240731_0017.raw', 'proAnubis_240716_2142.raw']\n",
      "['C://Users//jony//Programming//Python//Anubis//anubis//data//proAnubis_240731_0217.raw']\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import  os\n",
    "import glob\n",
    "\n",
    "# Add the directories to the sys.path\n",
    "dir_path = \"C://Users//jony//Programming//Python//Anubis//anubis//\" # insert your directory path\n",
    "sys.path.append(dir_path + \"Osiris//processing//python\")\n",
    "sys.path.append(dir_path + \"Osiris//monitoring//python\")\n",
    "sys.path.append(dir_path + \"tools\")\n",
    "\n",
    "import Analysis_tools as ATools\n",
    "import proAnubis_Analysis_Tools\n",
    "import Reconstruction_tools as RTools\n",
    "import rawEventBuilder\n",
    "import mplhep as hep\n",
    "import Timing_tools as TTools\n",
    "import rawFileReader\n",
    "import Visual_tools as VTools\n",
    "import datetime\n",
    "\n",
    "hep.style.use([hep.style.ATLAS])\n",
    "\n",
    "# Specify the directory\n",
    "data_list = sorted([f for f in os.listdir(\"data\") if os.path.isfile(os.path.join(\"data\", f))], reverse=True) ##all files in data directory sorted from the newest to the oldest\n",
    "print(data_list)\n",
    "file_path = [dir_path+\"//data//\"+\"proAnubis_240731_0217.raw\"] #\n",
    "file_path = list(map(lambda p: dir_path+\"data//\"+p, data_list))[2:3] # insert your file\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'Reconstruction_tools' has no attribute 'abs_badVsgood_hits_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#fReader.skip_events(10_000_000)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m initial_event_chunk \u001b[38;5;241m=\u001b[39m fReader\u001b[38;5;241m.\u001b[39mget_aligned_events(interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mRTools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs_badVsgood_hits_count\u001b[49m(initial_event_chunk))\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#VTools.time_events(initial_event_chunk)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#VTools.hitHeatMap(initial_event_chunk,0)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'Reconstruction_tools' has no attribute 'abs_badVsgood_hits_count'"
     ]
    }
   ],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "importlib.reload(VTools)\n",
    "\n",
    "fReader = rawFileReader.fileReader(file_path[0]) # load in the classs object\n",
    "#fReader.skip_events(10_000_000)\n",
    "initial_event_chunk = fReader.get_aligned_events(interval=100)\n",
    "print(RTools.abs_badVsgood_hits_count(initial_event_chunk))\n",
    "\n",
    "#VTools.time_events(initial_event_chunk)\n",
    "#VTools.hitHeatMap(initial_event_chunk,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing proAnubis_240731_0217:  98%|█████████▊| 975/1000 [00:39<00:01, 24.84Events/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "importlib.reload(VTools)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "order = [(0,1), (1,2), (2,3), (3,4)] # Order what you want to align\n",
    "actual_mets = {}\n",
    "last_reset = None\n",
    "for file in file_path:\n",
    "    max_process_event_chunk = 10_00 # End the loop early\n",
    "    file_name = file.split(\"//\")[-1][:-4]\n",
    "    fReader = rawFileReader.fileReader(file) # reload in the classs object\n",
    "    processedEvents = 0 # Initialisation\n",
    "\n",
    "    #Initialise variables to store the results\n",
    "    mets = [[] for pair in range(len(order))]\n",
    "    tdc_event_count = [[[],[]] for tdc in range(5)]# prepare for histogram plotting\n",
    "    initial_event_chunk = fReader.get_aligned_events(order=order, interval=interval) # get the initial event chunk\n",
    "    #originTime = max([event_chunk[0].tdcEvents[tdc].time for tdc in range(5) if event_chunk[0].tdcEvents[tdc].time])\n",
    "    #print(originTime)\n",
    "    binsx = []\n",
    "    max_chunk_duration = datetime.timedelta(0)\n",
    "    with tqdm(total=max_process_event_chunk, desc=f\"Processing {file_name}\", unit='Events') as pbar:\n",
    "        while processedEvents < max_process_event_chunk:\n",
    "            processedEvents += 1\n",
    "            try:\n",
    "                event_chunk = fReader.get_aligned_events(order=order, interval=interval) # get the aligned events\n",
    "                if not event_chunk:\n",
    "                    #print(\"Misaligned:\", processedEvents)\n",
    "                    for i, (tdc1, tdc2) in enumerate(order):\n",
    "                        mets[i].append(0) #I assume it is a zero but might not be right\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(\"Exception:\", e)\n",
    "                max_process_event_chunk = processedEvents-1\n",
    "                break\n",
    "            \n",
    "            event_time = max([event_chunk[0].tdcEvents[tdc].time for tdc in range(5) if event_chunk[0].tdcEvents[tdc].time])\n",
    "            \n",
    "            if last_reset:\n",
    "                if (event_time - last_reset).total_seconds() > 60:\n",
    "                    last_reset = event_time\n",
    "                    fReader.evtBuilder = rawEventBuilder.eventBuilder()\n",
    "            else:\n",
    "                last_reset = event_time\n",
    "                originTime = event_time\n",
    "            \n",
    "            for i, (tdc1, tdc2) in enumerate(order):\n",
    "                mets[i].append(VTools.metric_possible(event_chunk, tdc1, tdc2)[0])\n",
    "            tdc_event_count_buffer = RTools.abs_badVsgood_hits(event_chunk) # finding the number o\n",
    "            \n",
    "            last_event_time = max([event_chunk[-1].tdcEvents[tdc].time for tdc in range(5) if event_chunk[-1].tdcEvents[tdc].time])\n",
    "            first_event_time = max([event_chunk[0].tdcEvents[tdc].time for tdc in range(5) if event_chunk[0].tdcEvents[tdc].time])\n",
    "           # print(\"First event time:\", first_event_time )\n",
    "           # print(first_event_time - originTime)\n",
    "            #print((first_event_time - originTime).total_seconds())\n",
    "            binsx.append(processedEvents)\n",
    "            \"\"\"\n",
    "            if binsx:\n",
    "                binsx.append((first_event_time - originTime).total_seconds())\n",
    "            else:\n",
    "                originTime = first_event_time\n",
    "                binsx.append((first_event_time - originTime).total_seconds()) # stupid ik\n",
    "            if last_event_time - first_event_time > max_chunk_duration:\n",
    "                max_chunk_duration = last_event_time - first_event_time\n",
    "            \"\"\"\n",
    "            for tdc in  range(5):\n",
    "                tdc_event_count[tdc][0].append(tdc_event_count_buffer[tdc][0])\n",
    "                tdc_event_count[tdc][1].append(tdc_event_count_buffer[tdc][1])\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    #print(f\"Max Chunk Duration: {max_chunk_duration}\")\n",
    "    binsx = sorted(binsx)\n",
    "    actual_mets[file_name] = tdc_event_count\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    for tdc in range(0,5):\n",
    "        met = tdc_event_count[tdc]\n",
    "        ax.plot(binsx, met[0], label=f'Good hits TDC{tdc}')\n",
    "        ax.plot(binsx, met[1], label=f'Bad hits TDC{tdc}')\n",
    "\n",
    "    #ax.set_xlim(0, max_process_event_chunk)\n",
    "    # ax.set_ylim(-1, 100)\n",
    "    ax.legend()\n",
    "    ax.set_title('Abs Good vs Bad')\n",
    "    ax.set_ylabel('num of events')\n",
    "    #ax.set_xlabel('Absolute time / s')\n",
    "    plt.savefig(f\"hits_real_time_{file_name}.png\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    x_axis = [i for i in range(max_process_event_chunk)]\n",
    "    for i, pair in enumerate(order):\n",
    "        ax.plot( x_axis, mets[i], label=f'TDC{pair[0]} vs TDC{pair[1]}')\n",
    "    ax.legend()\n",
    "    ax.set_title('Possible alignement data count')\n",
    "    ax.set_ylabel('num of events')\n",
    "    ax.set_xlabel('Event Chunk number')\n",
    "    plt.savefig(f\"possible_{file_name}.png\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Chunk Duration: 0:00:08.958249\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max Chunk Duration: {max_chunk_duration}\")\n",
    "binsx = sorted(binsx)\n",
    "actual_mets[file_name] = tdc_event_count\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "for tdc in range(0,5):\n",
    "    met = tdc_event_count[tdc]\n",
    "    ax.plot(binsx, met[0], label=f'Good hits TDC{tdc}')\n",
    "    ax.plot(binsx, met[1], label=f'Bad hits TDC{tdc}')\n",
    "\n",
    "    #ax.set_xlim(0, max_process_event_chunk)\n",
    "    # ax.set_ylim(-1, 100)\n",
    "ax.legend()\n",
    "ax.set_title('Abs Good vs Bad')\n",
    "ax.set_ylabel('num of events')\n",
    "ax.set_xlabel('Absolute time / s')\n",
    "plt.savefig(f\"hits_real_time_{file_name}.png\")\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "x_axis = [i for i in range(max_process_event_chunk)]\n",
    "for i, pair in enumerate(order):\n",
    "    ax.plot( x_axis, mets[i], label=f'TDC{pair[0]} vs TDC{pair[1]}')\n",
    "ax.legend()\n",
    "ax.set_title('Possible alignement data count')\n",
    "ax.set_ylabel('num of events')\n",
    "ax.set_xlabel('Event Chunk number')\n",
    "plt.savefig(f\"possible_{file_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "with open('count_huge.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(tdc_event_count)\n",
    "\n",
    "with open('poss_huge.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(mets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing proAnubis_240810_0040: 100%|█████████▉| 2989/3000 [01:02<00:00, 47.52Events/s]\n"
     ]
    }
   ],
   "source": [
    "#Step plot plates\n",
    "\n",
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "importlib.reload(VTools)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "order = [(0,1), (1,2), (2,3), (3,4)] # Order what you want to align\n",
    "last_reset = None\n",
    "for file in file_path:\n",
    "    max_process_event_chunk = 3_000 # End the loop early\n",
    "    file_name = file.split(\"//\")[-1][:-4]\n",
    "    fReader = rawFileReader.fileReader(file) # reload in the classs object\n",
    "    processedEvents = 0 # Initialisation\n",
    "\n",
    "    #Initialise variables to store the results\n",
    "    counts = []#[0 for count in range(6)]\n",
    "    norm_counts = [] #normalized to percentage\n",
    "    initial_event_chunk = fReader.get_aligned_events(order=order, interval=interval) # get the initial event chunk\n",
    "    #originTime = max([event_chunk[0].tdcEvents[tdc].time for tdc in range(5) if event_chunk[0].tdcEvents[tdc].time])\n",
    "    #print(originTime)\n",
    "    binsx = []\n",
    "    max_chunk_duration = datetime.timedelta(0)\n",
    "    with tqdm(total=max_process_event_chunk, desc=f\"Processing {file_name}\", unit='Events') as pbar:\n",
    "        while processedEvents < max_process_event_chunk:\n",
    "            processedEvents += 1\n",
    "            try:\n",
    "                event_chunk = fReader.get_aligned_events(order=order, interval=interval) # get the aligned events\n",
    "                if not event_chunk:\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(\"Exception:\", e)\n",
    "                max_process_event_chunk = processedEvents\n",
    "                break\n",
    "            \n",
    "            event_time = max([event_chunk[0].tdcEvents[tdc].time for tdc in range(5) if event_chunk[0].tdcEvents[tdc].time])\n",
    "            \n",
    "            if last_reset:\n",
    "                if (event_time - last_reset).total_seconds() > 60:\n",
    "                    last_reset = event_time\n",
    "                    fReader.evtBuilder = rawEventBuilder.eventBuilder()\n",
    "            else:\n",
    "                last_reset = event_time\n",
    "                originTime = event_time\n",
    "            \n",
    "            chunk_counts = [0 for count in range(6)]\n",
    "            for evt_number, evt in enumerate(event_chunk):\n",
    "                rpc_activated = set()\n",
    "                for tdc in range(5):\n",
    "                    for word in evt.tdcEvents[tdc].words:\n",
    "                        rpc, hit = ATools.tdcChanToRPCHit(word, tdc, evt_number)\n",
    "                        if hit.eta:\n",
    "                            rpc_activated.add(rpc)\n",
    "                if not rpc_activated:\n",
    "                    #print(\"No record:\", processedEvents*100 + evt_number)\n",
    "                    VTools.all_hits_event(evt)\n",
    "                #print(rpc_activated)\n",
    "                #print(len(rpc_activated))\n",
    "                chunk_counts[len(rpc_activated)-1] += 1\n",
    "\n",
    "            counts.append(chunk_counts)\n",
    "            norm_counts.append([count/sum(chunk_counts) for count in chunk_counts])\n",
    "            pbar.update(1)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    #ax.hist(counts, bins = [i+0.5 for i in range(-1, 7)], histtype='step', label='RPCs activated')\n",
    "    #ax.set_xlim(0, max_process_event_chunk)\n",
    "    # ax.set_ylim(-1, 100)\n",
    "    for i in range(6):\n",
    "        ax.plot([count[i] for count in norm_counts], label=f'{i+1} RPCs activated')\n",
    "    ax.legend()\n",
    "    ax.set_title('Counts of activated RPCs')\n",
    "    ax.set_ylabel('num of events')\n",
    "    ax.set_xlabel('Count')\n",
    "    plt.show()\n",
    "    plt.savefig(f\"rpc_activated_{file_name}.png\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
