{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['proAnubis_240811_2240.raw', 'proAnubis_240810_2240.raw', 'proAnubis_240810_2040.raw', 'proAnubis_240810_1840.raw', 'proAnubis_240810_1640.raw', 'proAnubis_240810_1440.raw', 'proAnubis_240810_1240.raw', 'proAnubis_240810_1040.raw', 'proAnubis_240810_0840.raw', 'proAnubis_240810_0640.raw', 'proAnubis_240810_0440.raw', 'proAnubis_240810_0240.raw', 'proAnubis_240810_0040.raw']\n",
      "['C://Users//jony//Programming//Python//Anubis//anubis//data//reset//proAnubis_240810_0240.raw', 'C://Users//jony//Programming//Python//Anubis//anubis//data//reset//proAnubis_240810_0040.raw']\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import  os\n",
    "import glob\n",
    "\n",
    "# Add the directories to the sys.path\n",
    "dir_path = \"C://Users//jony//Programming//Python//Anubis//anubis//\" # insert your directory path\n",
    "sys.path.append(dir_path + \"Osiris//processing//python\")\n",
    "sys.path.append(dir_path + \"Osiris//monitoring//python\")\n",
    "sys.path.append(dir_path + \"tools\")\n",
    "\n",
    "import Analysis_tools as ATools\n",
    "import proAnubis_Analysis_Tools\n",
    "import Reconstruction_tools as RTools\n",
    "import rawEventBuilder\n",
    "import mplhep as hep\n",
    "import Timing_tools as TTools\n",
    "import rawFileReader\n",
    "import Visual_tools as VTools\n",
    "import datetime\n",
    "\n",
    "hep.style.use([hep.style.ATLAS])\n",
    "\n",
    "# Specify the directory\n",
    "data_list = sorted([f for f in os.listdir(\"data//reset\") if os.path.isfile(os.path.join(\"data//reset\", f))], reverse=True) ##all files in data directory sorted from the newest to the oldest\n",
    "print(data_list)\n",
    "file_path = [dir_path+\"//data//\"+\"proAnubis_240731_0217.raw\"] #\n",
    "file_path = list(map(lambda p: dir_path+\"data//reset//\"+p, data_list))[-1:] # insert your file\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[298, 46], [271, 47], [274, 14], [246, 42], [144, 55]]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "importlib.reload(VTools)\n",
    "\n",
    "fReader = rawFileReader.fileReader(file_path[0]) # load in the classs object\n",
    "#fReader.skip_events(10_000_000)\n",
    "initial_event_chunk = fReader.get_aligned_events(interval=100)\n",
    "print(RTools.abs_badVsgood_hits_count(initial_event_chunk))\n",
    "\n",
    "#VTools.time_events(initial_event_chunk)\n",
    "#VTools.hitHeatMap(initial_event_chunk,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing proAnubis_240811_2240:  63%|██████▎   | 313106/500000 [5:17:34<3:09:33, 16.43Events/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad Block Read\n",
      "Exception: list index out of range\n",
      "Max Chunk Duration: 0:01:07.582140\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (330958,) and (330957,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 92\u001b[0m\n\u001b[0;32m     90\u001b[0m x_axis \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_process_event_chunk)]\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, pair \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(order):\n\u001b[1;32m---> 92\u001b[0m     \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTDC\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m vs TDC\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m ax\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[0;32m     94\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPossible alignement data count\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jony\\Programming\\Python\\Anubis\\anubis\\.venv\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\jony\\Programming\\Python\\Anubis\\anubis\\.venv\\lib\\site-packages\\matplotlib\\axes\\_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jony\\Programming\\Python\\Anubis\\anubis\\.venv\\lib\\site-packages\\matplotlib\\axes\\_base.py:486\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    483\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    490\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (330958,) and (330957,)"
     ]
    }
   ],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "importlib.reload(VTools)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "order = [(0,1), (1,2), (2,3), (3,4)] # Order what you want to align\n",
    "actual_mets = {}\n",
    "last_reset = None\n",
    "for file in file_path:\n",
    "    max_process_event_chunk = 500_000 # End the loop early\n",
    "    file_name = file.split(\"//\")[-1][:-4]\n",
    "    fReader = rawFileReader.fileReader(file) # reload in the classs object\n",
    "    processedEvents = 0 # Initialisation\n",
    "\n",
    "    #Initialise variables to store the results\n",
    "    mets = [[] for pair in range(len(order))]\n",
    "    tdc_event_count = [[[],[]] for tdc in range(5)]# prepare for histogram plotting\n",
    "    initial_event_chunk = fReader.get_aligned_events(order=order, interval=interval) # get the initial event chunk\n",
    "    #originTime = max([event_chunk[0].tdcEvents[tdc].time for tdc in range(5) if event_chunk[0].tdcEvents[tdc].time])\n",
    "    #print(originTime)\n",
    "    binsx = []\n",
    "    max_chunk_duration = datetime.timedelta(0)\n",
    "    with tqdm(total=max_process_event_chunk, desc=f\"Processing {file_name}\", unit='Events') as pbar:\n",
    "        while processedEvents < max_process_event_chunk:\n",
    "            processedEvents += 1\n",
    "            try:\n",
    "                event_chunk = fReader.get_aligned_events(order=order, interval=interval) # get the aligned events\n",
    "                if not event_chunk:\n",
    "                    #print(\"Misaligned:\", processedEvents)\n",
    "                    for i, (tdc1, tdc2) in enumerate(order):\n",
    "                        mets[i].append(0) #I assume it is a zero but might not be right\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(\"Exception:\", e)\n",
    "                max_process_event_chunk = processedEvents\n",
    "                break\n",
    "            \n",
    "            event_time = max([event_chunk[0].tdcEvents[tdc].time for tdc in range(5) if event_chunk[0].tdcEvents[tdc].time])\n",
    "            \n",
    "            if last_reset:\n",
    "                if (event_time - last_reset).total_seconds() > 60:\n",
    "                    last_reset = event_time\n",
    "                    fReader.evtBuilder = rawEventBuilder.eventBuilder()\n",
    "            else:\n",
    "                last_reset = event_time\n",
    "                originTime = event_time\n",
    "            \n",
    "            for i, (tdc1, tdc2) in enumerate(order):\n",
    "                mets[i].append(VTools.metric_possible(event_chunk, tdc1, tdc2)[0])\n",
    "            tdc_event_count_buffer = RTools.abs_badVsgood_hits(event_chunk) # finding the number o\n",
    "            \n",
    "            last_event_time = max([event_chunk[-1].tdcEvents[tdc].time for tdc in range(5) if event_chunk[-1].tdcEvents[tdc].time])\n",
    "            first_event_time = max([event_chunk[0].tdcEvents[tdc].time for tdc in range(5) if event_chunk[0].tdcEvents[tdc].time])\n",
    "           # print(\"First event time:\", first_event_time )\n",
    "           # print(first_event_time - originTime)\n",
    "            #print((first_event_time - originTime).total_seconds())\n",
    "            if binsx:\n",
    "                binsx.append((first_event_time - originTime).total_seconds())\n",
    "            else:\n",
    "                originTime = first_event_time\n",
    "                binsx.append((first_event_time - originTime).total_seconds()) # stupid ik\n",
    "            if last_event_time - first_event_time > max_chunk_duration:\n",
    "                max_chunk_duration = last_event_time - first_event_time\n",
    "            for tdc in  range(5):\n",
    "                tdc_event_count[tdc][0].append(tdc_event_count_buffer[tdc][0])\n",
    "                tdc_event_count[tdc][1].append(tdc_event_count_buffer[tdc][1])\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    print(f\"Max Chunk Duration: {max_chunk_duration}\")\n",
    "    binsx = sorted(binsx)\n",
    "    actual_mets[file_name] = tdc_event_count\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    for tdc in range(0,5):\n",
    "        met = tdc_event_count[tdc]\n",
    "        ax.plot(binsx, met[0], label=f'Good hits TDC{tdc}')\n",
    "        ax.plot(binsx, met[1], label=f'Bad hits TDC{tdc}')\n",
    "\n",
    "    #ax.set_xlim(0, max_process_event_chunk)\n",
    "    # ax.set_ylim(-1, 100)\n",
    "    ax.legend()\n",
    "    ax.set_title('Abs Good vs Bad')\n",
    "    ax.set_ylabel('num of events')\n",
    "    ax.set_xlabel('Absolute time / s')\n",
    "    plt.savefig(f\"hits_real_time_{file_name}.png\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    x_axis = [i for i in range(max_process_event_chunk)]\n",
    "    for i, pair in enumerate(order):\n",
    "        ax.plot( x_axis, mets[i], label=f'TDC{pair[0]} vs TDC{pair[1]}')\n",
    "    ax.legend()\n",
    "    ax.set_title('Possible alignement data count')\n",
    "    ax.set_ylabel('num of events')\n",
    "    ax.set_xlabel('Event Chunk number')\n",
    "    plt.savefig(f\"possible_{file_name}.png\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Chunk Duration: 0:01:07.582140\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max Chunk Duration: {max_chunk_duration}\")\n",
    "binsx = sorted(binsx)\n",
    "actual_mets[file_name] = tdc_event_count\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "for tdc in range(0,5):\n",
    "    met = tdc_event_count[tdc]\n",
    "    ax.plot(binsx, met[0], label=f'Good hits TDC{tdc}')\n",
    "    ax.plot(binsx, met[1], label=f'Bad hits TDC{tdc}')\n",
    "\n",
    "    #ax.set_xlim(0, max_process_event_chunk)\n",
    "    # ax.set_ylim(-1, 100)\n",
    "ax.legend()\n",
    "ax.set_title('Abs Good vs Bad')\n",
    "ax.set_ylabel('num of events')\n",
    "ax.set_xlabel('Absolute time / s')\n",
    "plt.savefig(f\"hits_real_time_{file_name}.png\")\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "x_axis = [i for i in range(max_process_event_chunk)]\n",
    "for i, pair in enumerate(order):\n",
    "    ax.plot( x_axis[:-1], mets[i], label=f'TDC{pair[0]} vs TDC{pair[1]}')\n",
    "ax.legend()\n",
    "ax.set_title('Possible alignement data count')\n",
    "ax.set_ylabel('num of events')\n",
    "ax.set_xlabel('Event Chunk number')\n",
    "plt.savefig(f\"possible_{file_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "with open('count_huge.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(tdc_event_count)\n",
    "\n",
    "with open('poss_huge.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(mets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing proAnubis_240810_0240: 100%|██████████| 1000/1000 [00:27<00:00, 36.77Events/s]\n",
      "Processing proAnubis_240810_0040: 100%|█████████▉| 997/1000 [00:24<00:00, 40.55Events/s]\n"
     ]
    }
   ],
   "source": [
    "#Step plot plates\n",
    "\n",
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "importlib.reload(VTools)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "order = [(0,1), (1,2), (2,3), (3,4)] # Order what you want to align\n",
    "last_reset = None\n",
    "for file in file_path:\n",
    "    max_process_event_chunk = 3_000 # End the loop early\n",
    "    file_name = file.split(\"//\")[-1][:-4]\n",
    "    fReader = rawFileReader.fileReader(file) # reload in the classs object\n",
    "    processedEvents = 0 # Initialisation\n",
    "\n",
    "    #Initialise variables to store the results\n",
    "    counts = []#[0 for count in range(6)]\n",
    "    norm_counts = [] #normalized to percentage\n",
    "    initial_event_chunk = fReader.get_aligned_events(order=order, interval=interval) # get the initial event chunk\n",
    "    #originTime = max([event_chunk[0].tdcEvents[tdc].time for tdc in range(5) if event_chunk[0].tdcEvents[tdc].time])\n",
    "    #print(originTime)\n",
    "    binsx = []\n",
    "    max_chunk_duration = datetime.timedelta(0)\n",
    "    with tqdm(total=max_process_event_chunk, desc=f\"Processing {file_name}\", unit='Events') as pbar:\n",
    "        while processedEvents < max_process_event_chunk:\n",
    "            processedEvents += 1\n",
    "            try:\n",
    "                event_chunk = fReader.get_aligned_events(order=order, interval=interval) # get the aligned events\n",
    "                if not event_chunk:\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(\"Exception:\", e)\n",
    "                max_process_event_chunk = processedEvents\n",
    "                break\n",
    "            \n",
    "            event_time = max([event_chunk[0].tdcEvents[tdc].time for tdc in range(5) if event_chunk[0].tdcEvents[tdc].time])\n",
    "            \n",
    "            if last_reset:\n",
    "                if (event_time - last_reset).total_seconds() > 60:\n",
    "                    last_reset = event_time\n",
    "                    fReader.evtBuilder = rawEventBuilder.eventBuilder()\n",
    "            else:\n",
    "                last_reset = event_time\n",
    "                originTime = event_time\n",
    "            \n",
    "            chunk_counts = [0 for count in range(6)]\n",
    "            for evt_number, evt in enumerate(event_chunk):\n",
    "                rpc_activated = set()\n",
    "                for tdc in range(5):\n",
    "                    for word in evt.tdcEvents[tdc].words:\n",
    "                        rpc, hit = ATools.tdcChanToRPCHit(word, tdc, evt_number)\n",
    "                        if hit.eta:\n",
    "                            rpc_activated.add(rpc)\n",
    "                if not rpc_activated:\n",
    "                    #print(\"No record:\", processedEvents*100 + evt_number)\n",
    "                    VTools.all_hits_event(evt)\n",
    "                #print(rpc_activated)\n",
    "                #print(len(rpc_activated))\n",
    "                chunk_counts[len(rpc_activated)-1] += 1\n",
    "\n",
    "            counts.append(chunk_counts)\n",
    "            norm_counts.append([count/sum(chunk_counts) for count in chunk_counts])\n",
    "            pbar.update(1)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    #ax.hist(counts, bins = [i+0.5 for i in range(-1, 7)], histtype='step', label='RPCs activated')\n",
    "    #ax.set_xlim(0, max_process_event_chunk)\n",
    "    # ax.set_ylim(-1, 100)\n",
    "    for i in range(6):\n",
    "        ax.plot([count[i] for count in counts], label=f'{i+1} RPCs activated')\n",
    "    ax.legend()\n",
    "    ax.set_title('Counts of activated RPCs')\n",
    "    ax.set_ylabel('num of events')\n",
    "    ax.set_xlabel('Count')\n",
    "    plt.show()\n",
    "    #plt.savefig(f\"rpc_activated_{file_name}.png\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
