{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['proAnubis_240810_2240.raw', 'proAnubis_240810_2040.raw', 'proAnubis_240810_1840.raw', 'proAnubis_240810_1640.raw', 'proAnubis_240810_1440.raw', 'proAnubis_240810_1240.raw', 'proAnubis_240810_1040.raw', 'proAnubis_240810_0840.raw', 'proAnubis_240810_0640.raw', 'proAnubis_240810_0440.raw', 'proAnubis_240810_0240.raw', 'proAnubis_240810_0040.raw']\n",
      "['C://Users//jony//Programming//Python//Anubis//anubis//data//reset//proAnubis_240810_1440.raw']\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import  os\n",
    "import glob\n",
    "import rawEventBuilder\n",
    "# Add the directories to the sys.path\n",
    "dir_path = \"C://Users//jony//Programming//Python//Anubis//anubis//\" # insert your directory path\n",
    "sys.path.append(dir_path + \"Osiris//processing//python\")\n",
    "sys.path.append(dir_path + \"Osiris//monitoring//python\")\n",
    "sys.path.append(dir_path + \"tools\")\n",
    "\n",
    "import Analysis_tools as ATools\n",
    "import proAnubis_Analysis_Tools\n",
    "import Reconstruction_tools as RTools\n",
    "import mplhep as hep\n",
    "import Timing_tools as TTools\n",
    "import rawFileReader\n",
    "import Visual_tools as VTools\n",
    "import datetime\n",
    "\n",
    "hep.style.use([hep.style.ATLAS])\n",
    "\n",
    "# Specify the directory\n",
    "data_list = sorted([f for f in os.listdir(\"data//reset\") if os.path.isfile(os.path.join(\"data//reset\", f))], reverse=True) ##all files in data directory sorted from the newest to the oldest\n",
    "print(data_list)\n",
    "file_path = [dir_path+\"//data//\"+\"proAnubis_240731_0217.raw\"] #\n",
    "file_path = list(map(lambda p: dir_path+\"data//reset//\"+p, data_list))[-1:] # insert your file\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[298, 46], [271, 47], [274, 14], [246, 42], [144, 55]]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "importlib.reload(VTools)\n",
    "\n",
    "fReader = rawFileReader.fileReader(file_path[0]) # load in the classs object\n",
    "#fReader.skip_events(10_000_000)\n",
    "initial_event_chunk = fReader.get_aligned_events(interval=100)\n",
    "print(RTools.abs_badVsgood_hits_count(initial_event_chunk))\n",
    "\n",
    "#VTools.time_events(initial_event_chunk)\n",
    "#VTools.hitHeatMap(initial_event_chunk,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing proAnubis_240810_1440:  52%|█████▏    | 2098/4000 [01:04<00:35, 53.07Events/s]"
     ]
    }
   ],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "importlib.reload(VTools)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "order = [(0,1), (1,2), (2,3), (3,4)] # Order what you want to align\n",
    "actual_mets = {}\n",
    "last_reset = None\n",
    "for file in file_path:\n",
    "    max_process_event_chunk = 4_000 # End the loop early\n",
    "    file_name = file.split(\"//\")[-1][:-4]\n",
    "    fReader = rawFileReader.fileReader(file) # reload in the classs object\n",
    "    processedEvents = 0 # Initialisation\n",
    "\n",
    "    #Initialise variables to store the results\n",
    "    mets = [[] for pair in range(len(order))]\n",
    "    tdc_event_count = [[[],[]] for tdc in range(5)]# prepare for histogram plotting\n",
    "    initial_event_chunk = fReader.get_aligned_events(order=order, interval=interval) # get the initial event chunk\n",
    "    #originTime = max([event_chunk[0].tdcEvents[tdc].time for tdc in range(5) if event_chunk[0].tdcEvents[tdc].time])\n",
    "    #print(originTime)\n",
    "    binsx = []\n",
    "    max_chunk_duration = datetime.timedelta(0)\n",
    "    with tqdm(total=max_process_event_chunk, desc=f\"Processing {file_name}\", unit='Events') as pbar:\n",
    "        while processedEvents < max_process_event_chunk:\n",
    "            processedEvents += 1\n",
    "            try:\n",
    "                event_chunk = fReader.get_aligned_events(order=order, interval=interval) # get the aligned events\n",
    "                if not event_chunk:\n",
    "                    #print(\"Misaligned:\", processedEvents)\n",
    "                    for i, (tdc1, tdc2) in enumerate(order):\n",
    "                        mets[i].append(0) #I assume it is a zero but might not be right\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(\"Exception:\", e)\n",
    "                max_process_event_chunk = processedEvents\n",
    "                break\n",
    "            \n",
    "            event_time = max([event_chunk[0].tdcEvents[tdc].time for tdc in range(5) if event_chunk[0].tdcEvents[tdc].time])\n",
    "            \n",
    "            if last_reset:\n",
    "                if (event_time - last_reset).total_seconds() > 60:\n",
    "                    last_reset = event_time\n",
    "                    fReader.evtBuilder = rawEventBuilder.eventBuilder()\n",
    "            else:\n",
    "                last_reset = event_time\n",
    "                originTime = event_time\n",
    "            \n",
    "            for i, (tdc1, tdc2) in enumerate(order):\n",
    "                mets[i].append(VTools.metric_possible(event_chunk, tdc1, tdc2)[0])\n",
    "            tdc_event_count_buffer = RTools.abs_badVsgood_hits(event_chunk) # finding the number o\n",
    "            \n",
    "            last_event_time = max([event_chunk[-1].tdcEvents[tdc].time for tdc in range(5) if event_chunk[-1].tdcEvents[tdc].time])\n",
    "            first_event_time = max([event_chunk[0].tdcEvents[tdc].time for tdc in range(5) if event_chunk[0].tdcEvents[tdc].time])\n",
    "           # print(\"First event time:\", first_event_time )\n",
    "           # print(first_event_time - originTime)\n",
    "            #print((first_event_time - originTime).total_seconds())\n",
    "            if binsx:\n",
    "                binsx.append((first_event_time - originTime).total_seconds())\n",
    "            else:\n",
    "                originTime = first_event_time\n",
    "                binsx.append((first_event_time - originTime).total_seconds()) # stupid ik\n",
    "            if last_event_time - first_event_time > max_chunk_duration:\n",
    "                max_chunk_duration = last_event_time - first_event_time\n",
    "            for tdc in  range(5):\n",
    "                tdc_event_count[tdc][0].append(tdc_event_count_buffer[tdc][0])\n",
    "                tdc_event_count[tdc][1].append(tdc_event_count_buffer[tdc][1])\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \"\"\"\n",
    "    print(f\"Max Chunk Duration: {max_chunk_duration}\")\n",
    "    binsx = sorted(binsx)\n",
    "    actual_mets[file_name] = tdc_event_count\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    for tdc in range(0,5):\n",
    "        met = tdc_event_count[tdc]\n",
    "        ax.plot(binsx, met[0], label=f'Good hits TDC{tdc}')\n",
    "        ax.plot(binsx, met[1], label=f'Bad hits TDC{tdc}')\n",
    "\n",
    "    #ax.set_xlim(0, max_process_event_chunk)\n",
    "    # ax.set_ylim(-1, 100)\n",
    "    ax.legend()\n",
    "    ax.set_title('Abs Good vs Bad')\n",
    "    ax.set_ylabel('num of events')\n",
    "    ax.set_xlabel('Absolute time / s')\n",
    "    plt.savefig(f\"hits_real_time_{file_name}.png\")\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    x_axis = [i for i in range(max_process_event_chunk)]\n",
    "    for i, pair in enumerate(order):\n",
    "        ax.plot( x_axis, mets[i], label=f'TDC{pair[0]} vs TDC{pair[1]}')\n",
    "    ax.legend()\n",
    "    ax.set_title('Possible alignement data count')\n",
    "    ax.set_ylabel('num of events')\n",
    "    ax.set_xlabel('Event Chunk number')\n",
    "    plt.savefig(f\"possible_{file_name}.png\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max Chunk Duration: {max_chunk_duration}\")\n",
    "print(\"binsx\", binsx)\n",
    "binsx = sorted(binsx)\n",
    "actual_mets[file_name] = tdc_event_count\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "for tdc in range(0,5):\n",
    "    met = tdc_event_count[tdc]\n",
    "    ax.plot(binsx, met[0], label=f'Good hits TDC{tdc}')\n",
    "    ax.plot(binsx, met[1], label=f'Bad hits TDC{tdc}')\n",
    "\n",
    "#ax.set_xlim(0, max_process_event_chunk)\n",
    "# ax.set_ylim(-1, 100)\n",
    "ax.legend()\n",
    "ax.set_title('Abs Good vs Bad')\n",
    "ax.set_ylabel('num of events')\n",
    "ax.set_xlabel('Absolute time / s')\n",
    "plt.savefig(f\"hits_real_time_{file_name}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
