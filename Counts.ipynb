{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['proAnubis_240818_1925.raw', 'proAnubis_240810_1240.raw', 'proAnubis_240810_0840.raw', 'proAnubis_240627_1720.raw', 'datafiles.root']\n",
      "['C://Users//jony//Programming//Python//Anubis//anubis//data//proAnubis_240818_1925.raw']\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import  os\n",
    "import glob\n",
    "\n",
    "# Add the directories to the sys.path\n",
    "dir_path = \"C://Users//jony//Programming//Python//Anubis//anubis//\" # insert your directory path\n",
    "sys.path.append(dir_path + \"Osiris//processing//python\")\n",
    "sys.path.append(dir_path + \"Osiris//monitoring//python\")\n",
    "sys.path.append(dir_path + \"tools\")\n",
    "\n",
    "import Analysis_tools as ATools\n",
    "import proAnubis_Analysis_Tools\n",
    "import Reconstruction_tools as RTools\n",
    "import rawEventBuilder\n",
    "import mplhep as hep\n",
    "import Timing_tools as TTools\n",
    "import rawFileReader\n",
    "import Visual_tools as VTools\n",
    "import datetime\n",
    "\n",
    "hep.style.use([hep.style.ATLAS])\n",
    "\n",
    "# Specify the directory\n",
    "data_list = sorted([f for f in os.listdir(\"data\") if os.path.isfile(os.path.join(\"data\", f))], reverse=True) ##all files in data directory sorted from the newest to the oldest\n",
    "print(data_list)\n",
    "file_path = [dir_path+\"data//\"+\"proAnubis_240810_1240.raw\", dir_path+\"data//\"+\"proAnubis_240818_1925.raw\"][1:] #\n",
    "#file_path = list(map(lambda p: dir_path+\"data//\"+p, data_list))[:1] # insert your file\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'Reconstruction_tools' has no attribute 'abs_badVsgood_hits_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#fReader.skip_events(10_000_000)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m initial_event_chunk \u001b[38;5;241m=\u001b[39m fReader\u001b[38;5;241m.\u001b[39mget_aligned_events(interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mRTools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs_badVsgood_hits_count\u001b[49m(initial_event_chunk))\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#VTools.time_events(initial_event_chunk)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#VTools.hitHeatMap(initial_event_chunk,0)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'Reconstruction_tools' has no attribute 'abs_badVsgood_hits_count'"
     ]
    }
   ],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "importlib.reload(VTools)\n",
    "\n",
    "fReader = rawFileReader.fileReader(file_path[0]) # load in the classs object\n",
    "#fReader.skip_events(10_000_000)\n",
    "initial_event_chunk = fReader.get_aligned_events(interval=100)\n",
    "print(RTools.abs_badVsgood_hits_count(initial_event_chunk))\n",
    "\n",
    "#VTools.time_events(initial_event_chunk)\n",
    "#VTools.hitHeatMap(initial_event_chunk,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping Events:   0%|          | 0/10000 [00:00<?, ?Events/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping Events: 100%|██████████| 10000/10000 [00:00<00:00, 13071.09Events/s]\n",
      "Processing proAnubis_240818_1925:   8%|▊         | 823/10000 [00:19<03:32, 43.13Events/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m processedEvents \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     event_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mfReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_aligned_events\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterval\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get the aligned events\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m event_chunk:\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;66;03m#print(\"Misaligned:\", processedEvents)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, (tdc1, tdc2) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(order):\n",
      "File \u001b[1;32mC:\\/Users//jony//Programming//Python//Anubis//anubis//Osiris//processing//python\\rawFileReader.py:94\u001b[0m, in \u001b[0;36mfileReader.get_aligned_events\u001b[1;34m(self, order, interval, extract_tdc_mets)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtdc_monitoring_event_buffer\u001b[38;5;241m.\u001b[39mextend(evts_chunk)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtdc_monitoring_counter \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2500\u001b[39m:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m#print(self.tdc_monitoring_counter)\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     TDC_error_time, tdc_mets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonitor_tdc_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecordtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monly_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtdc_monitoring_event_buffer\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtdc_monitoring_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mC:\\/Users//jony//Programming//Python//Anubis//anubis//Osiris//processing//python\\rawFileReader.py:-1\u001b[0m, in \u001b[0;36mfileReader.monitor_tdc_state\u001b[1;34m(self, recordtimes, only_min)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "importlib.reload(VTools)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "order = [(0,1), (1,2), (2,3), (3,4)] # Order what you want to align\n",
    "actual_mets = {}\n",
    "last_reset = None\n",
    "for file in file_path:\n",
    "    max_process_event_chunk = 10_000 # End the loop early\n",
    "    file_name = file.split(\"//\")[-1][:-4]\n",
    "    fReader = rawFileReader.fileReader(file) # reload in the classs object\n",
    "    processedEvents = 0 # Initialisation\n",
    "\n",
    "    #Initialise variables to store the results\n",
    "    mets = [[] for pair in range(len(order))]\n",
    "    tdc_event_count = [[[],[]] for tdc in range(5)]# prepare for histogram plotting\n",
    "    initial_event_chunk = fReader.get_aligned_events(order=order, interval=interval) # get the initial event chunk\n",
    "    #originTime = max([event_chunk[0].tdcEvents[tdc].time for tdc in range(5) if event_chunk[0].tdcEvents[tdc].time])\n",
    "    #print(originTime)\n",
    "    binsx = []\n",
    "    max_chunk_duration = datetime.timedelta(0)\n",
    "    with tqdm(total=max_process_event_chunk, desc=f\"Processing {file_name}\", unit='Events') as pbar:\n",
    "        while processedEvents < max_process_event_chunk:\n",
    "            processedEvents += 1\n",
    "            try:\n",
    "                event_chunk = fReader.get_aligned_events(order=order, interval=interval) # get the aligned events\n",
    "                if not event_chunk:\n",
    "                    #print(\"Misaligned:\", processedEvents)\n",
    "                    for i, (tdc1, tdc2) in enumerate(order):\n",
    "                        mets[i].append(0) #I assume it is a zero but might not be right\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(\"Exception:\", e)\n",
    "                max_process_event_chunk = processedEvents-1\n",
    "                break\n",
    "            \n",
    "            event_time = max([event_chunk[0].tdcEvents[tdc].time for tdc in range(5) if event_chunk[0].tdcEvents[tdc].time])\n",
    "            \n",
    "            if last_reset:\n",
    "                if (event_time - last_reset).total_seconds() > 60:\n",
    "                    last_reset = event_time\n",
    "                    fReader.evtBuilder = rawEventBuilder.eventBuilder()\n",
    "            else:\n",
    "                last_reset = event_time\n",
    "                originTime = event_time\n",
    "            \n",
    "            if event_time - originTime > datetime.timedelta(350):\n",
    "                continue\n",
    "            for i, (tdc1, tdc2) in enumerate(order):\n",
    "                mets[i].append(VTools.metric_possible(event_chunk, tdc1, tdc2)[0])\n",
    "            tdc_event_count_buffer = RTools.abs_badVsgood_hits(event_chunk) # finding the number o\n",
    "            \n",
    "            last_event_time = max([event_chunk[-1].tdcEvents[tdc].time for tdc in range(5) if event_chunk[-1].tdcEvents[tdc].time])\n",
    "            first_event_time = max([event_chunk[0].tdcEvents[tdc].time for tdc in range(5) if event_chunk[0].tdcEvents[tdc].time])\n",
    "           # print(\"First event time:\", first_event_time )\n",
    "           # print(first_event_time - originTime)\n",
    "            #print((first_event_time - originTime).total_seconds())\n",
    "            binsx.append((event_time - originTime).total_seconds())\n",
    "            \"\"\"\n",
    "            if binsx:\n",
    "                binsx.append((first_event_time - originTime).total_seconds())\n",
    "            else:\n",
    "                originTime = first_event_time\n",
    "                binsx.append((first_event_time - originTime).total_seconds()) # stupid ik\n",
    "            if last_event_time - first_event_time > max_chunk_duration:\n",
    "                max_chunk_duration = last_event_time - first_event_time\n",
    "            \"\"\"\n",
    "            for tdc in  range(5):\n",
    "                tdc_event_count[tdc][0].append(tdc_event_count_buffer[tdc][0])\n",
    "                tdc_event_count[tdc][1].append(tdc_event_count_buffer[tdc][1])\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    #print(f\"Max Chunk Duration: {max_chunk_duration}\")\n",
    "    binsx = sorted(binsx)\n",
    "    actual_mets[file_name] = tdc_event_count\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    for tdc in range(0,5):\n",
    "        met = tdc_event_count[tdc]\n",
    "        ax.plot(binsx, met[0], label=f'Good hits TDC{tdc}')\n",
    "        ax.plot(binsx, met[1], label=f'Bad hits TDC{tdc}')\n",
    "\n",
    "    #ax.set_xlim(0, max_process_event_chunk)\n",
    "    # ax.set_ylim(-1, 100)\n",
    "    ax.legend()\n",
    "    ax.set_title('Abs Good vs Bad')\n",
    "    ax.set_ylabel('num of events')\n",
    "    #ax.set_xlabel('Absolute time / s')\n",
    "    plt.savefig(f\"hits_real_time_{file_name}.png\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    x_axis = [i for i in range(max_process_event_chunk)]\n",
    "    for i, pair in enumerate(order):\n",
    "        ax.plot( x_axis, mets[i], label=f'TDC{pair[0]} vs TDC{pair[1]}')\n",
    "    ax.legend()\n",
    "    ax.set_title('Possible alignement data count')\n",
    "    ax.set_ylabel('num of events')\n",
    "    ax.set_xlabel('Event Chunk number')\n",
    "    plt.savefig(f\"possible_{file_name}.png\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Chunk Duration: 0:00:08.958249\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max Chunk Duration: {max_chunk_duration}\")\n",
    "binsx = sorted(binsx)\n",
    "actual_mets[file_name] = tdc_event_count\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "for tdc in range(0,5):\n",
    "    met = tdc_event_count[tdc]\n",
    "    ax.plot(binsx, met[0], label=f'Good hits TDC{tdc}')\n",
    "    ax.plot(binsx, met[1], label=f'Bad hits TDC{tdc}')\n",
    "\n",
    "    #ax.set_xlim(0, max_process_event_chunk)\n",
    "    # ax.set_ylim(-1, 100)\n",
    "ax.legend()\n",
    "ax.set_title('Abs Good vs Bad')\n",
    "ax.set_ylabel('num of events')\n",
    "ax.set_xlabel('Absolute time / s')\n",
    "plt.savefig(f\"hits_real_time_{file_name}.png\")\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "x_axis = [i for i in range(max_process_event_chunk)]\n",
    "for i, pair in enumerate(order):\n",
    "    ax.plot( x_axis, mets[i], label=f'TDC{pair[0]} vs TDC{pair[1]}')\n",
    "ax.legend()\n",
    "ax.set_title('Possible alignement data count')\n",
    "ax.set_ylabel('num of events')\n",
    "ax.set_xlabel('Event Chunk number')\n",
    "plt.savefig(f\"possible_{file_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "with open('count_huge.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(tdc_event_count)\n",
    "\n",
    "with open('poss_huge.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(mets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing proAnubis_240818_1925:  75%|███████▍  | 7483/10000 [06:52<02:18, 18.16Events/s]  \n"
     ]
    }
   ],
   "source": [
    "#Step plot plates\n",
    "\n",
    "importlib.reload(rawFileReader) # Reload fReader\n",
    "importlib.reload(proAnubis_Analysis_Tools)\n",
    "importlib.reload(ATools)\n",
    "importlib.reload(VTools)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "interval = 100 # Set your monitoring chunck size\n",
    "order = [(0,1), (1,2), (2,3), (3,4)] # Order what you want to align\n",
    "last_reset = None\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "for file in file_path:\n",
    "    max_process_event_chunk = 10_000 # End the loop early\n",
    "    file_name = file.split(\"//\")[-1][:-4]\n",
    "    fReader = rawFileReader.fileReader(file) # reload in the classs object\n",
    "    processedEvents = 0 # Initialisation\n",
    "    #Initialise variables to store the results\n",
    "    counts = []#[0 for count in range(6)]\n",
    "    norm_counts = [] #normalized to percentage\n",
    "    avg = [] #average number of activated rpcs\n",
    "    histogram = [] #histogram of activated rpcs\n",
    "    fake_histogram = []\n",
    "    initial_event_chunk = fReader.get_aligned_events(order=order, interval=interval) # get the initial event chunk\n",
    "    #originTime = max([event_chunk[0].tdcEvents[tdc].time for tdc in range(5) if event_chunk[0].tdcEvents[tdc].time])\n",
    "    #print(originTime)\n",
    "    binsx = []\n",
    "    max_chunk_duration = datetime.timedelta(0)\n",
    "    with tqdm(total=max_process_event_chunk, desc=f\"Processing {file_name}\", unit='Events') as pbar:\n",
    "        while processedEvents < max_process_event_chunk:\n",
    "            processedEvents += 1\n",
    "            try:\n",
    "                event_chunk = fReader.get_aligned_events(order=order, interval=interval) # get the aligned events\n",
    "                if not event_chunk:\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(\"Exception:\", e)\n",
    "                max_process_event_chunk = processedEvents\n",
    "                break\n",
    "            \n",
    "            event_time = max([event_chunk[0].tdcEvents[tdc].time for tdc in range(5) if event_chunk[0].tdcEvents[tdc].time])\n",
    "            \n",
    "            if last_reset:\n",
    "                if (event_time - last_reset).total_seconds() > 60:\n",
    "                    last_reset = event_time\n",
    "                    fReader.evtBuilder = rawEventBuilder.eventBuilder()\n",
    "            else:\n",
    "                last_reset = event_time\n",
    "                originTime = event_time\n",
    "                #fReader.skip_events(10_000)\n",
    "            \n",
    "            chunk_counts = [0 for count in range(6)]\n",
    "            for evt_number, evt in enumerate(event_chunk):\n",
    "                rpc_activated = set()\n",
    "                for tdc in range(5):\n",
    "                    for word in evt.tdcEvents[tdc].words:\n",
    "                        rpc, hit = ATools.tdcChanToRPCHit(word, tdc, evt_number)\n",
    "                        if hit.eta:\n",
    "                            rpc_activated.add(rpc)\n",
    "                if not rpc_activated:\n",
    "                    #print(\"No record:\", processedEvents*100 + evt_number)\n",
    "                    VTools.all_hits_event(evt)\n",
    "                #print(rpc_activated)\n",
    "                #print(len(rpc_activated))\n",
    "                chunk_counts[len(rpc_activated)-1] += 1\n",
    "                if (event_time - originTime).total_seconds() > 350:\n",
    "                    histogram.append(len(rpc_activated))\n",
    "                else:\n",
    "                    fake_histogram.append(len(rpc_activated))\n",
    "            new_avg = 0\n",
    "            for i, count in enumerate(chunk_counts):\n",
    "                new_avg += i*count/sum(chunk_counts)\n",
    "            avg.append(new_avg) \n",
    "            counts.append(chunk_counts)\n",
    "            norm_counts.append([count/sum(chunk_counts) for count in chunk_counts])\n",
    "            pbar.update(1)\n",
    "    ax.hist(fake_histogram, bins = [i+0.5 for i in range(-1, 7)], histtype='step', label=f'RPCs activated before 350s')\n",
    "    ax.hist(histogram, bins = [i+0.5 for i in range(-1, 7)], histtype='step', label=f'RPCs activated after 350s')\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    #ax.set_xlim(0, max_process_event_chunk)\n",
    "    #ax.set_ylim(-1, 100)\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    for i in range(6):\n",
    "        ax.plot([count[i] for count in counts], label=f'{i+1} RPCs activated')\n",
    "    ax.legend()\n",
    "    ax.set_title('Counts of activated RPCs')\n",
    "    ax.set_ylabel('num of events')\n",
    "    ax.set_xlabel('Count')\n",
    "    plt.show()\n",
    "    plt.savefig(f\"rpc_activated_{file_name}.png\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.plot(avg, label='Average number of activated RPCs')\n",
    "    ax.legend()\n",
    "    ax.set_title('Average of Eta hits per event')\n",
    "    ax.set_xlabel('num of Event Chunks')\n",
    "    ax.set_ylabel('Count')\n",
    "    plt.show()\n",
    "    plt.savefig(f\"rpc_average_{file_name}.png\")\n",
    "    \"\"\"\n",
    "ax.legend()\n",
    "ax.set_title('Histogram of activated RPCs')\n",
    "ax.set_ylabel('num of events')\n",
    "ax.set_xlabel('Count')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.hist(fake_histogram, bins = [i+0.5 for i in range(-1, 7)], histtype='step', label=f'RPCs activated before 350s', density=True)\n",
    "ax.hist(histogram, bins = [i+0.5 for i in range(-1, 7)], histtype='step', label=f'RPCs activated after 350s', density=True)\n",
    "ax.legend()\n",
    "ax.set_title('Histogram of activated RPCs')\n",
    "ax.set_ylabel('num of events')\n",
    "ax.set_xlabel('Count')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
